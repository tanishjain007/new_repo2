{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2ca3259-9917-4fee-9668-2a80b8ae0010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 01 - Introduction to Databricks and Workspace\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to Databricks! This module introduces you to the Databricks platform, its workspace, and core concepts. Since you already know SQL, Python, Pandas, and PySpark, we'll focus on how Databricks enhances your existing skills.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "- What is Databricks and why it's important for data engineering\n",
    "- Databricks workspace navigation and structure\n",
    "- Creating and working with Databricks notebooks\n",
    "- Understanding clusters and compute resources\n",
    "- Basic operations in Databricks environment\n",
    "- Differences between Databricks and local PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1815f676-73bc-4e55-a05d-f307c082b1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What is Databricks?\n",
    "\n",
    "**Databricks** is a unified analytics platform built on Apache Spark that simplifies big data processing and machine learning. It's a cloud-native platform that provides:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Unified Analytics Platform**: Combines data engineering, data science, and business analytics in one place\n",
    "2. **Apache Spark Optimization**: Optimized Spark runtime that's 2-10x faster than standard Spark\n",
    "3. **Collaborative Workspace**: Share notebooks, dashboards, and insights with your team\n",
    "4. **Managed Infrastructure**: No need to manage clusters, Spark versions, or dependencies manually\n",
    "5. **Delta Lake Integration**: Built-in support for ACID transactions and time travel on data lakes\n",
    "6. **Multi-language Support**: Python, SQL, Scala, and R in the same notebook\n",
    "\n",
    "### Why Databricks for Data Engineers?\n",
    "\n",
    "- **Faster Development**: Pre-configured Spark clusters, no setup time\n",
    "- **Better Performance**: Optimized Spark engine (Databricks Runtime)\n",
    "- **Production Ready**: Built-in job scheduling, monitoring, and alerting\n",
    "- **Cloud Integration**: Native integration with AWS, Azure, and GCP storage\n",
    "- **Collaboration**: Teams can work together on notebooks and share results\n",
    "- **Cost Effective**: Auto-scaling clusters, pay only for what you use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4bfff33-d6f7-4d1a-aa33-bac73e27a92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Architecture Overview\n",
    "\n",
    "### Components\n",
    "\n",
    "1. **Control Plane**: Databricks-managed services (workspace, notebooks, jobs)\n",
    "2. **Data Plane**: Your cloud account (VPC, storage, compute)\n",
    "3. **Workspace**: Your collaborative environment (notebooks, dashboards, libraries)\n",
    "4. **Clusters**: Compute resources that run your Spark jobs\n",
    "5. **Jobs**: Scheduled or on-demand execution of notebooks or scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63f3a96-0210-40ba-b92a-29aeb936f341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Layers: Control Plane vs Data Plane\n",
    "\n",
    "- **Control plane (managed by Databricks)**: UI/API, notebooks, job scheduler, cluster manager, SQL endpoints, Unity Catalog metastore, audit logs, tokens/SCIM/IAM integration, secrets management.\n",
    "\n",
    "- **Data plane (your cloud account)**: compute instances (driver + workers), DBFS root (object storage mount), customer-owned data in object storage, VPC/VNet networking, private endpoints. Compute pulls config from the control plane but data never needs to transit the control plane.\n",
    "\n",
    "- **Implication**: security teams care about keeping PII in the data plane while still leveraging SaaS management; networking rules (PE/VPCE) control traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98df6b4-e824-49b4-951d-fd42aefb1d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Cloud Nuances (AWS, Azure, GCP)\n",
    "- **AWS**: data plane in your VPC; S3 for storage; IAM roles for passthrough; PrivateLink for control-plane + S3; Kinesis/Kafka integrations.\n",
    "\n",
    "- **Azure**: data plane in your VNet; ADLS Gen2; Managed Identities for passthrough; Private Link for CP/DP + storage firewall; AAD tokens.\n",
    "\n",
    "- **GCP**: data plane in your VPC; GCS; Service Accounts with short-lived tokens; Private Service Connect for control-plane; BigQuery connector optional.\n",
    "\n",
    "- **Serverless SQL**: lives in Databricks-owned account; networking and patching handled by Databricks; great for BI with minimal ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9449a2a-c2e6-476f-81ae-f169a0822de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Execution Flow (Notebook/Job → Data)\n",
    "\n",
    "1. User submits a notebook cell / job task via UI/API.\n",
    "\n",
    "2. Control plane authenticates, fetches metadata (cluster config, UC policies).\n",
    "\n",
    "3. Driver starts in data plane with config; workers attach; libraries pulled from DBFS/whl/pip mirror.\n",
    "\n",
    "4. Spark plan built (Catalyst) → optimized (AQE, DPP) → executed; I/O hits object storage via data-plane network.\n",
    "\n",
    "5. Results return to driver → UI/warehouse; lineage and audit persisted in control plane; logs/metrics emitted to storage/monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48b2ad6-e699-479a-89a7-530a0c656cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks vs Local PySpark\n",
    "\n",
    "| Feature | Local PySpark | Databricks |\n",
    "|---------|---------------|------------|\n",
    "| Setup | Manual installation | Pre-configured |\n",
    "| Cluster Management | Manual | Automatic |\n",
    "| Performance | Standard Spark | Optimized (2-10x faster) |\n",
    "| Collaboration | Limited | Built-in |\n",
    "| Storage | Local/Manual config | Integrated with cloud storage |\n",
    "| Monitoring | Basic | Advanced dashboards |\n",
    "| Cost | Fixed hardware | Pay-per-use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cd5a240-0385-424b-897b-1d585c898dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding the Databricks Workspace\n",
    "\n",
    "The workspace is your main interface in Databricks. It's organized into:\n",
    "\n",
    "### Workspace Structure\n",
    "\n",
    "1. **Workspace**: Your personal or shared folder structure\n",
    "   - `/Users/your_email@domain.com/` - Your personal workspace\n",
    "   - `/Shared/` - Shared with all users\n",
    "   - `/Repos/` - Git repositories\n",
    "\n",
    "2. **Notebooks**: Interactive documents with code and markdown\n",
    "   - Support multiple languages (Python, SQL, Scala, R)\n",
    "   - Can mix languages in the same notebook\n",
    "   - Support for widgets and parameters\n",
    "\n",
    "3. **Clusters**: Compute resources\n",
    "   - Single Node: For development (no distributed computing)\n",
    "   - Standard: Multi-node clusters for production\n",
    "   - High Concurrency: For SQL workloads\n",
    "\n",
    "4. **Jobs**: Scheduled or triggered tasks\n",
    "   - Run notebooks or JAR files\n",
    "   - Can be scheduled with cron expressions\n",
    "   - Support for job dependencies\n",
    "\n",
    "5. **SQL Warehouses**: For SQL analytics\n",
    "   - Serverless compute for SQL queries\n",
    "   - Auto-scaling and auto-termination\n",
    "\n",
    "6. **Libraries**: Python, JAR, or other dependencies\n",
    "   - Can be installed at cluster or notebook level\n",
    "   - Support for PyPI, Maven, CRAN, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1eabdd-a8c1-48d5-a239-30a206804931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster and Warehouse Architecture\n",
    "\n",
    "- **Driver**: runs SparkContext, notebook commands, job orchestration; talks to control plane for auth/config.\n",
    "\n",
    "- **Workers**: executors doing distributed compute; autoscaling adds/removes workers within min/max; spot/ondemand mix to save cost.\n",
    "\n",
    "- **SQL Warehouse**: managed compute tuned for BI; serverless variant lives fully in control plane account (no VPC needed) while pro/classic live in your VPC.\n",
    "\n",
    "- **Photon**: vectorized engine for SQL/DataFrame; accelerates I/O + compute without code change.\n",
    "\n",
    "- **Adaptive execution**: AQE, dynamic partition pruning, broadcast hints—all apply to SQL and PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83becfc3-5faf-4c7d-b5ed-2dd5a460ddb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating Your First Notebook\n",
    "\n",
    "In Databricks, you can create notebooks through the UI or programmatically. Let's explore the notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c89698e-af7c-4e16-aac5-0d3a7986c626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Databricks runtime version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3781cd-b477-4d38-be8d-0265a2ec4d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 1. Spark Version (This works fine)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# 2. Databricks Runtime Version (The reliable way)\n",
    "# This looks for the environment variable set on the worker/driver\n",
    "import os\n",
    "dbr_version = os.getenv(\"DATABRICKS_RUNTIME_VERSION\", \"N/A\")\n",
    "print(f\"Databricks Runtime: {dbr_version}\")\n",
    "\n",
    "# 3. App ID (Safe get with a default)\n",
    "# app_id = spark.conf.get(\"spark.app.id\", \"N/A\")\n",
    "# print(f\"App ID: {app_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc704edd-a2b1-40d9-8259-604d43cd3f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_version(), version();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27ea3a6-5909-4a2d-9ddc-a8fb580f939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check available Spark configurations\n",
    "print(\"Key Spark Configurations:\")\n",
    "configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in sorted(configs)[:20]:  # Show first 20\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d379fe43-5b6c-4b77-b23e-e9dbc9ac227c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Serverless compute (for notebooks, jobs, and SQL warehouses) uses the Spark Connect architecture. This is a modern, decoupled mode where:\n",
    "\n",
    "Your client (the notebook or warehouse frontend) talks to a remote Spark server via gRPC.\n",
    "There is no local Spark driver JVM running in your session.\n",
    "The traditional SparkContext (accessed via spark.sparkContext) lives on the driver in classic Spark — but it doesn't exist in the same way here.\n",
    "\n",
    "As a result, any code that tries to access spark.sparkContext (or related low-level driver/JVM attributes like getConf().getAll(), applicationId, hadoopConfiguration, parallelize, broadcast, RDD APIs, checkpointing via context, etc.) will fail with this exact [JVM_ATTRIBUTE_NOT_SUPPORTED] error (or similar ones like [CONFIG_NOT_AVAILABLE])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8818b51f-f4c5-40a2-905c-edb439cc2ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding Clusters\n",
    "\n",
    "Clusters are the compute resources in Databricks. They can be:\n",
    "\n",
    "### Cluster Types\n",
    "\n",
    "1. **All-Purpose Clusters**: For interactive development\n",
    "   - Multiple users can attach\n",
    "   - Manual start/stop\n",
    "   - Best for notebooks and ad-hoc analysis\n",
    "\n",
    "2. **Job Clusters**: For automated jobs\n",
    "   - Created automatically for jobs\n",
    "   - Terminated after job completion\n",
    "   - Cost-effective for scheduled tasks\n",
    "\n",
    "3. **SQL Warehouses**: For SQL analytics\n",
    "   - Serverless compute\n",
    "   - Auto-scaling\n",
    "   - Optimized for SQL queries\n",
    "\n",
    "### Cluster Components\n",
    "\n",
    "- **Driver Node**: Coordinates the cluster and runs your code\n",
    "- **Worker Nodes**: Execute tasks in parallel\n",
    "- **Databricks Runtime**: Optimized Spark distribution with pre-installed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f299c9-791c-436b-b043-4e9b57bf99c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get cluster information\n",
    "print(\"Cluster Information:\")\n",
    "print(f\"Number of cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Check if running on Databricks\n",
    "try:\n",
    "    dbutils_info = dbutils.notebook.entry_point.getDbutils()\n",
    "    print(\"\\nRunning on Databricks: Yes\")\n",
    "    print(f\"Notebook path: {dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()}\")\n",
    "except:\n",
    "    print(\"\\nRunning on Databricks: No (or dbutils not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9844a457-7657-45d3-851e-d239dbe7dd48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Utilities (dbutils)\n",
    "\n",
    "`dbutils` is a Databricks-specific utility that provides helper functions for:\n",
    "\n",
    "- **File system operations**: Working with DBFS (Databricks File System)\n",
    "- **Secrets management**: Accessing secure credentials\n",
    "- **Notebook workflows**: Running other notebooks\n",
    "- **Widgets**: Creating interactive parameters\n",
    "- **Data**: Mounting external storage\n",
    "\n",
    "### Key dbutils Commands\n",
    "\n",
    "```python\n",
    "# File system\n",
    "dbutils.fs.ls(\"/\")\n",
    "dbutils.fs.mkdirs(\"/tmp/demo\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "\n",
    "# Secrets\n",
    "dbutils.secrets.get(scope=\"my-scope\", key=\"my-key\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"path/to/notebook\", timeout_seconds=60)\n",
    "\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"input\", \"default_value\")\n",
    "dbutils.widgets.get(\"input\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9662da30-83ae-462c-9808-3b16f1f8720d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore dbutils file system\n",
    "print(\"DBFS Root Directory:\")\n",
    "files = dbutils.fs.ls(\"/\")\n",
    "for file in files:\n",
    "    print(f\"  {file.name:30s} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e91330-5988-4636-bbb2-c84e61309949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Databricks File System (DBFS) structure\n",
    "print(\"\\nDBFS Structure:\")\n",
    "print(\"\\n/dbfs - Databricks File System root\")\n",
    "print(\"/FileStore - Uploaded files and generated outputs\")\n",
    "print(\"/databricks - System files\")\n",
    "print(\"/tmp - Temporary files\")\n",
    "print(\"/mnt - Mount points for external storage\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81584ad9-1f9d-4600-a200-1dcf6535b484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Multiple Languages\n",
    "\n",
    "One of Databricks' powerful features is the ability to mix languages in a single notebook. You can switch between Python, SQL, Scala, and R seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed10fb8a-b944-4daf-a094-9ba5e7cffef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python cell - Create a sample DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create sample data\n",
    "data = [(\"Alice\", 25, \"Engineering\"),\n",
    "        (\"Bob\", 30, \"Sales\"),\n",
    "        (\"Charlie\", 35, \"Engineering\"),\n",
    "        (\"Diana\", 28, \"Marketing\")]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "print(\"DataFrame created and registered as temporary view 'employees'\")\n",
    "df.show()\n",
    "\n",
    "print(\"Using df.display() in Databricks to show output instead of df.show() as used in PySpark\")\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9c13a0-7827-42b2-9b3b-4693a5ed56aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's query the same data using SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82400b5b-0701-43af-95df-000d76e92525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL cell - Query the temporary view\n",
    "SELECT \n",
    "    department,\n",
    "    COUNT(*) as employee_count,\n",
    "    AVG(age) as avg_age\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "ORDER BY employee_count DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc0b513-9258-44bf-925c-e4817643b236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook Magic Commands\n",
    "\n",
    "Databricks supports magic commands for notebook operations:\n",
    "\n",
    "- `%python` - Run Python code\n",
    "- `%sql` - Run SQL code\n",
    "- `%scala` - Run Scala code\n",
    "- `%r` - Run R code\n",
    "- `%sh` - Run shell commands\n",
    "- `%md` - Markdown cell\n",
    "- `%run` - Run another notebook\n",
    "- `%fs` - File system operations (shortcut for dbutils.fs)\n",
    "- `%pip` - Install Python packages\n",
    "- `%sql` - SQL queries\n",
    "\n",
    "### Example: Using Magic Commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5398b496-6f00-4894-9d87-fa907746e9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "\n",
    "# Using %fs magic command (alternative to dbutils.fs)\n",
    "\n",
    "ls\n",
    "\n",
    "\n",
    "# The %fs magic command (and almost all dbutils.fs commands) is completely disabled when your notebook is attached to a serverless SQL warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ea3630-0b34-4dab-b25b-7897fcb39520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List files in a volume\n",
    "LIST 'dbfs:/Volumes/retail_catalog/v01/retail-pipeline';\n",
    "\n",
    "-- Or more detailed\n",
    "SELECT path, modificationTime, length \n",
    "FROM read_files(\n",
    "  '/Volumes/retail_catalog/v01/retail-pipeline',\n",
    "  format => 'binaryFile'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9755d62-42cd-4413-bbb5-53514821cb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Using %sh for shell commands\n",
    "\n",
    "echo \"Hello from shell\" && date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71f228c-4c42-45a8-94d1-65f43f9febf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Differences: Databricks vs Local PySpark\n",
    "\n",
    "### 1. SparkSession Initialization\n",
    "\n",
    "**Local PySpark**:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.some.config\", \"value\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "**Databricks**:\n",
    "```python\n",
    "# SparkSession is already created and optimized\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Or simply use the pre-configured 'spark' object\n",
    "```\n",
    "\n",
    "### 2. File System Access\n",
    "\n",
    "**Local PySpark**:\n",
    "```python\n",
    "df = spark.read.csv(\"file:///path/to/file.csv\")\n",
    "```\n",
    "\n",
    "**Databricks**:\n",
    "```python\n",
    "# Use DBFS paths\n",
    "df = spark.read.csv(\"/FileStore/tables/file.csv\")\n",
    "# Or mount external storage\n",
    "df = spark.read.csv(\"/mnt/adls/data/file.csv\")\n",
    "```\n",
    "\n",
    "### 3. Library Management\n",
    "\n",
    "**Local PySpark**:\n",
    "```python\n",
    "# Install via pip, manage manually\n",
    "!pip install pandas\n",
    "```\n",
    "\n",
    "**Databricks**:\n",
    "```python\n",
    "# Install at cluster or notebook level\n",
    "%pip install pandas\n",
    "# Or use cluster libraries (persistent across restarts)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5384c3e8-371d-4df6-a160-e1300f85e6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Create a simple DataFrame and compare with Pandas\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'score': [85, 90, 88, 92, 87]\n",
    "})\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_df)\n",
    "print(f\"\\nPandas DataFrame size: {pandas_df.shape}\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"\\nSpark DataFrame:\")\n",
    "spark_df.show()\n",
    "print(f\"\\nSpark DataFrame count: {spark_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a5a3519-d477-4ff1-9cc4-d3a0b96dca50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices for Databricks Notebooks\n",
    "\n",
    "1. **Use %md cells** for documentation and explanations\n",
    "2. **Organize code logically** - separate data ingestion, transformation, and output\n",
    "3. **Use widgets** for parameterization\n",
    "4. **Leverage temporary views** to share data between Python and SQL cells\n",
    "5. **Use display()** instead of show() for better visualization\n",
    "6. **Clean up resources** - unpersist DataFrames when done\n",
    "7. **Use cluster libraries** for frequently used packages\n",
    "8. **Version control** - use Git integration for notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4cfe5e0-cc4b-4bb7-9224-217e3b794c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Using display() for better visualization\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "# Create sample sales data\n",
    "sales_data = [\n",
    "    (\"2024-01-01\", \"Product A\", 100.0, \"North\"),\n",
    "    (\"2024-01-01\", \"Product B\", 150.0, \"South\"),\n",
    "    (\"2024-01-02\", \"Product A\", 120.0, \"North\"),\n",
    "    (\"2024-01-02\", \"Product C\", 200.0, \"East\"),\n",
    "    (\"2024-01-03\", \"Product B\", 180.0, \"South\"),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"date\", \"product\", \"amount\", \"region\"])\n",
    "\n",
    "# Use display() for interactive tables (better than show())\n",
    "display(sales_df)\n",
    "\n",
    "# Aggregated view\n",
    "summary = sales_df.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4643ac4-8062-4d18-87af-9596b46c4b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "✅ **What Databricks is** - A unified analytics platform built on Apache Spark\n",
    "\n",
    "✅ **Workspace structure** - How Databricks organizes notebooks, clusters, and jobs\n",
    "\n",
    "✅ **Notebook basics** - Creating and working with multi-language notebooks\n",
    "\n",
    "✅ **Clusters** - Understanding compute resources in Databricks\n",
    "\n",
    "✅ **dbutils** - Databricks-specific utilities for file operations and more\n",
    "\n",
    "✅ **Key differences** - How Databricks differs from local PySpark\n",
    "\n",
    "✅ **Best practices** - Tips for effective notebook development\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- Data ingestion from various sources\n",
    "- Working with DBFS and external storage\n",
    "- Integrating with Azure Data Lake Storage Gen2\n",
    "- Reading and writing different file formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41333135-62aa-4f89-85a0-4c6045a13654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Try these exercises to practice:\n",
    "\n",
    "1. Create a new notebook and explore the dbutils.fs commands\n",
    "2. Create a Spark DataFrame from a Python list and convert it to a Pandas DataFrame\n",
    "3. Use both Python and SQL cells to analyze the same dataset\n",
    "4. Use the display() function to visualize a DataFrame with at least 100 rows\n",
    "5. Check your cluster configuration and note the number of cores available\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5296116087534777,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_introduction_workspace",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
